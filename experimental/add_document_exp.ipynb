{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test adding french docs using chapters of le petite prince."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of docs:  27\n",
      "average doc size:  61.4444444444\n",
      "number of docs:  1\n",
      "average doc size:  8.0\n"
     ]
    }
   ],
   "source": [
    "# First parse testing documents\n",
    "file_name = 'le_petite_prince.txt'\n",
    "f = open(file_name, 'r')\n",
    "fr_docs = []\n",
    "for line in f:\n",
    "    if line.isupper():\n",
    "        fr_docs.append([])\n",
    "    fr_docs[-1].append(line)\n",
    "print \"number of docs: \", len(fr_docs)\n",
    "print \"average doc size: \", sum([len(d) for d in fr_docs])/float(len(fr_docs))\n",
    "file_name = 'zh_news.txt'\n",
    "f = open(file_name, 'r')\n",
    "zh_docs = [[]]\n",
    "for line in f:\n",
    "    zh_docs[0].append(line)\n",
    "print \"number of docs: \", len(zh_docs)\n",
    "print \"average doc size: \", sum([len(d) for d in zh_docs])/float(len(zh_docs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eyJhbGciOiJIUzI1NiJ9.eyJlbWFpbCI6ImtlcmVuLmd1LjEwQGdtYWlsLmNvbSJ9.LvQcwQDgAIC6ThhNxQchRcActo6cXrO6x-5ezy406tM\n"
     ]
    }
   ],
   "source": [
    "# Second get token\n",
    "import json, requests\n",
    "email = 'keren.gu.10@gmail.com'\n",
    "payload = {\n",
    "    'email': email,\n",
    "    'password': 'ugnerek',\n",
    "}\n",
    "r = requests.post('http://localhost:5001/api/authenticate-user', data=payload)\n",
    "token = json.loads(r.content)['result']['token']\n",
    "print token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Third make POST calls\n",
    "def post_add_document(doc, lang):\n",
    "    payload = {\n",
    "        'title': doc[0],\n",
    "        'text': \"\\t\".join(doc[1:]),\n",
    "        'lang': lang\n",
    "    }\n",
    "    r = requests.post('http://localhost:5001/api/add-document?auth_token={}&email={}'.format(token, email),\\\n",
    "                      data=payload)\n",
    "    return r\n",
    "    #json.loads(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Baseline Chinese\n",
    "for doc in zh_docs: \n",
    "    r = post_add_document(doc, 'zh')\n",
    "    json.loads(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'num_excerpts': 13, u'num_phrases_found': 0, u'avg_get_ph_time': 13.725701900628897, u'success': 1, u'document_id': u'5682d5dfba8fcd5170147862'}\n"
     ]
    }
   ],
   "source": [
    "print json.loads(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREMIER CHAPITRE 25.1564149857\n",
      "25.1564149857\n"
     ]
    }
   ],
   "source": [
    "# French\n",
    "import sys\n",
    "responses = []\n",
    "ts = [] # track time per document\n",
    "for doc in fr_docs[0:1]:\n",
    "    start = time.time()\n",
    "    r = post_add_document(doc, 'fr')\n",
    "    responses.append(json.loads(r.content))\n",
    "    ts.append(time.time() - start)\n",
    "    print doc[0].strip(), ts[-1]\n",
    "    sys.stdout.flush()\n",
    "print sum(ts) / float(len(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total phrases count: 402 hit count:  246\n",
      "total hit rate: 0.611940298507\n",
      "average approx hit rate:  0.611940298507\n",
      "unique missed phrases:  106\n",
      ", 1, 2, abandonné, afin, amélioré, avalait, avalent, aventures, avions, boa, boas, bouger, bridge, c'est, calcul, carrière, chapeau, chine, comprennent, compréhensive, conseillé, contacts, contente, copie, couleur, cravates, crayon, d'explications, d'oeil, d'oeuvre, d'étoiles, dessin, dessins, dessiné, digestion, digérait, dorment, découragé, entière, explications, faisais, fatigant, fauve, feraitil, fermés, forêt, forêts, golf, grammaire, grandes, géographie, histoires, j'ai, j'avais, j'en, jungle, l'arizona, l'expérience, l'histoire, l'insuccès, l'intérieur, l'on, l'âge, lucide, m'a, m'intéresser, m'ont, mettais, montré, mâcher, métier, n'a, no1, opinion, ouverts, paraissait, parlais, peinture, piloter, politique, portée, proie, puissent, raisonnable, reconnaître, rencontrais, représentait, réfléchi, répondait, répondu, s'appelait, serpent, serpents, servi, seules, tas, tracer, utile, vierge, vierges, vues, vécu, vécues, égaré, éléphant, "
     ]
    }
   ],
   "source": [
    "# Go over the response and get 1) the list of uniq missed phrases, 2) hit rate, 3) average time per exerpts\n",
    "missed_phrases = set()\n",
    "hit_rates = 0\n",
    "total_phrases = 0\n",
    "total_hit = 0\n",
    "for r in responses:\n",
    "    missed_phrases |= set(r['missed_phrases'])\n",
    "    hit_rates += r['total_hit_rate']\n",
    "    total_phrases += r['total_phrase_count']\n",
    "    total_hit += r['num_phrases_found']\n",
    "print \"total phrases count:\", total_phrases, \"hit count: \", total_hit\n",
    "print \"total hit rate:\", total_hit / float(total_phrases)\n",
    "print \"average approx hit rate: \", hit_rates/float(len(responses))\n",
    "print \"unique missed phrases: \", len(missed_phrases)\n",
    "for ph in sorted(list(missed_phrases)):\n",
    "    sys.stdout.write(ph + \", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mongodb://tlemberg:tlemberg@glossify.io/tenk\n",
      "250000\n"
     ]
    }
   ],
   "source": [
    "import dbutils\n",
    "db = dbutils.DBConnect('glossify.io', 'tlemberg', 'tlemberg')\n",
    "coll = db[\"phrases_es\"]\n",
    "# print coll.find_one({'base': 'contact', 'txs': {'$exists': 1}}, {'_id': 1})\n",
    "print coll.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
